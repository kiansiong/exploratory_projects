{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. CS5242_Kaggle_02Nov2019.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aBrgOSOO7X4M",
        "qqVZ1ppoU2mr",
        "OUZ4irXC_b1_",
        "Wzxk6zsF9iCd",
        "5BA5sRvquCIo",
        "PwGWYMmVkeE3",
        "Z_eGihaWNFAk"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiansiong/exploratory_projects/blob/master/DeepLearning1_MalwareAnalysis_CNN_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oCUFSbwh1o6b",
        "colab_type": "code",
        "outputId": "e32ea145-4091-4158-9858-4f6043ecc5b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from pytz import timezone\n",
        "\n",
        "# import tensorflow as tf\n",
        "from keras import metrics\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras import backend\n",
        "from keras.layers import GRU, Conv1D, Dense, AveragePooling1D, MaxPooling1D, Dropout, Input, concatenate, LSTM\n",
        "from keras.callbacks import EarlyStopping, Callback, LearningRateScheduler, TensorBoard, ReduceLROnPlateau, ModelCheckpoint, History\n",
        "from keras import regularizers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoI9NtCgiQjF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#zip archive !need to check why this does work\n",
        "zipFile = \"/content/drive/My Drive/NUS_2019/cs5242_kaggle/data/cs5242-neural-networks-and-deep-learning.zip\"\n",
        "\n",
        "#file paths\n",
        "dataPath_train = '/content/kaggle/train/'\n",
        "dataPath_test = '/content/kaggle/test/'\n",
        "target = '/content/kaggle/train_kaggle.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXs7DwLgzW89",
        "colab_type": "code",
        "outputId": "03b89a1c-1496-4daf-b34c-6597d7f133f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#mount drive to access files. if files are deleted, have to unzip from 'My Drive' again. \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBrgOSOO7X4M",
        "colab_type": "text"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6QDgAuHzyCF",
        "colab_type": "code",
        "outputId": "68aef141-1b40-4ca4-b6e3-b94cc1e03586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!mkdir /content/kaggle\n",
        "!ls \"/content/drive/My Drive/NUS_2019/cs5242_kaggle/data\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/kaggle’: File exists\n",
            "cs5242-neural-networks-and-deep-learning.zip\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RXkF4DgPxLU",
        "colab_type": "text"
      },
      "source": [
        "**Manually download zip file from Kaggle, and upload to My Drive. Then unzip**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bx7ev04zyHj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture \n",
        "!unzip \"/content/drive/My Drive/NUS_2019/cs5242_kaggle/data/cs5242-neural-networks-and-deep-learning.zip\" -d \"/content/kaggle\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW3_wTsZTVnZ",
        "colab_type": "code",
        "outputId": "7017df07-8e07-4182-8724-00632639986c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#move unzipped files to working directory\n",
        "!mv /content/kaggle/train/train/*.npy /content/kaggle/train\n",
        "!mv /content/kaggle/test/test/*.npy /content/kaggle/test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mv: cannot stat '/content/kaggle/train/train/*.npy': No such file or directory\n",
            "mv: cannot stat '/content/kaggle/test/test/*.npy': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DBcMKN38WLI",
        "colab_type": "code",
        "outputId": "4ffe1279-1c87-4cc6-8de8-0562ff4de32d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#remove empty folder\n",
        "!rmdir /content/kaggle/train/train\n",
        "!rmdir /content/kaggle/test/test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rmdir: failed to remove '/content/kaggle/train/train': No such file or directory\n",
            "rmdir: failed to remove '/content/kaggle/test/test': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tejttt258PSl",
        "colab_type": "code",
        "outputId": "a3ac7e04-a26e-4d4e-cf2f-9a79520b6f90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#check file count is correct. train=18662, test=6051\n",
        "!ls /content/kaggle/train | wc -l\n",
        "!ls /content/kaggle/test | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18662\n",
            "6051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uogyvq7rDaKd",
        "colab_type": "code",
        "outputId": "61f5851e-8982-420d-fa27-3c256172f8e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#top 5 training files\n",
        "!ls -lrt /content/kaggle/train | head -5\n",
        "\n",
        "#top 5 test files\n",
        "!ls -lrt /content/kaggle/test | head -5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4344112\n",
            "-rw-r--r-- 1 root root  43376 Sep 10 05:00 1.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 05:00 10.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 05:00 101.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 05:00 1010.npy\n",
            "total 1071112\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 04:58 1.npy\n",
            "-rw-r--r-- 1 root root  30728 Sep 10 04:58 10.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 04:58 100.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 04:58 1005.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rf63nVRxSh7t",
        "colab_type": "text"
      },
      "source": [
        "# Initialise and Check Data\n",
        "Also to check if data unzipped correctly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8kQseHj4mLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# data = pd.read_csv(target, header=0, sep=',', quotechar='\"')\n",
        "# data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ6e_RC4QDmf",
        "colab_type": "code",
        "outputId": "b6cc2317-615e-4a84-c4bf-26c8624cd579",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!ls -lrt /content/kaggle/train | head -5"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4344112\n",
            "-rw-r--r-- 1 root root  43376 Sep 10 05:00 1.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 05:00 10.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 05:00 101.npy\n",
            "-rw-r--r-- 1 root root 408128 Sep 10 05:00 1010.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S-9H41gQsue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_load = np.load(dataPath_train+'1.npy')\n",
        "# print(test_load.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQCTQLeSRImS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test_load[:5,:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiYxAdAPUQRf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # to load numpy file type\n",
        "# sampleTrain = np.load(dataPath_train + '11.npy')\n",
        "# sampleTrain.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6azcMqFmU2Ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read all training sequence for all instances\n",
        "path = '/content/kaggle/train/'\n",
        "sequences = list()\n",
        "\n",
        "for i in range(0, len(listdir(path))):\n",
        "    file_name = path + str(i) + '.npy'\n",
        "    data = np.load(file_name)\n",
        "    df = pd.DataFrame(data=data[:,:], index=None, columns=None)\n",
        "    df = df.drop(df.iloc[:,64:92].head(0).columns, axis=1)\n",
        "    values = df.values\n",
        "    sequences.append(values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTfXQzdgU5_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the label\n",
        "target = pd.read_csv('/content/kaggle/train_kaggle.csv')\n",
        "target = target.values[:,1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUuI_sySU8Qs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # preprocessing steps\n",
        "# len_sequences = []\n",
        "\n",
        "# for seq in sequences:\n",
        "#     len_sequences.append(len(seq))\n",
        "    \n",
        "# pd.Series(len_sequences).describe()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTainMqDXlnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x = np.array(len_sequences)\n",
        "# print(np.unique(x, return_counts=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBMzv_ePVDNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # 90% quartile is 1000 length, so the rest need to pad up to 1000 length\n",
        "# pd.Series(len_sequences).quantile(q=0.9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33GjUEWmerz7",
        "colab_type": "code",
        "outputId": "520274af-61f4-461d-ee0a-b030798c5336",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "'''\n",
        "# dont pad\n",
        "final_seq = np.array(sequences)\n",
        "final_seq.shape\n",
        "final_seq[1].shape\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# dont pad\\nfinal_seq = np.array(sequences)\\nfinal_seq.shape\\nfinal_seq[1].shape\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TikpziOVD5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding the sequence with values from last row for each sequence\n",
        "# since most of the sequence are 1000 length, no need to truncate further after padding\n",
        "\n",
        "to_pad = 1000\n",
        "new_sequences = []\n",
        "for seq in sequences:\n",
        "    len_seq = len(seq)\n",
        "    # last_val = seq[-1]\n",
        "    last_val = [0]*74\n",
        "    n = to_pad - len_seq\n",
        "    '''\n",
        "    to_concat = np.repeat(seq[-1], n).reshape(102, n).transpose()\n",
        "    new_seq = np.concatenate([seq, to_concat])\n",
        "    new_sequences.append(new_seq)\n",
        "    '''\n",
        "    to_concat = np.repeat(last_val, n).reshape(74, n).transpose()\n",
        "    new_seq = np.concatenate([seq, to_concat])\n",
        "    new_sequences.append(new_seq)\n",
        "    \n",
        "final_seq = np.stack(new_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOTdg81LM7_D",
        "colab_type": "code",
        "outputId": "8c370c0d-ea2c-4e48-8f7a-5e58d674a40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#resize all, pad 0s\n",
        "new_sequences = []\n",
        "\n",
        "for seq in sequences:\n",
        "  new_seq = seq.copy()\n",
        "  new_seq.resize((1000,102), refcheck=False)\n",
        "  new_sequences.append(new_seq)\n",
        "\n",
        "final_seq = np.stack(new_sequences)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#resize all, pad 0s\\nnew_sequences = []\\n\\nfor seq in sequences:\\n  new_seq = seq.copy()\\n  new_seq.resize((1000,102), refcheck=False)\\n  new_sequences.append(new_seq)\\n\\nfinal_seq = np.stack(new_sequences)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_7bE58-QcIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# #check that sequences are padded to 0 \n",
        "# new_sequences[11][57]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf0PC-2zn4Eh",
        "colab_type": "code",
        "outputId": "6d974562-b2fb-44be-86ee-3ec8f41b37b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#clear memory \n",
        "import gc\n",
        "sequences = None\n",
        "len_sequences = None\n",
        "new_sequences = None\n",
        "history = None\n",
        "predict_data = None\n",
        "model = None\n",
        "visible = None\n",
        "left_conv1 = None\n",
        "right_conv1 = None\n",
        "merge = None\n",
        "gru = None\n",
        "output = None\n",
        "gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTkFFZoHVF45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split the data into training, test, validation\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "\n",
        "'''\n",
        "X_train, X_test, y_train, y_test = train_test_split(final_seq, target, test_size=0.2, random_state=1)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)\n",
        "X_valid = np.array(X_valid)\n",
        "y_valid = np.array(y_valid)\n",
        "'''\n",
        "\n",
        "#k fold CV\n",
        "k=10\n",
        "folds = list(StratifiedKFold(n_splits=k, shuffle=True, random_state=None).split(final_seq, target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwL_JLS3hBhq",
        "colab_type": "code",
        "outputId": "0eb47a2d-a37c-4053-f381-4e755e697b70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "'''\n",
        "#Check results of k-fold split\n",
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "    print('\\nFold ',j)\n",
        "    print('train_idx = ',train_idx)\n",
        "    print('val_idx = ',val_idx)\n",
        "    X_train_cv = final_seq[train_idx]\n",
        "    y_train_cv = target[train_idx]\n",
        "    X_valid_cv = final_seq[val_idx]\n",
        "    y_valid_cv = target[val_idx]\n",
        "\n",
        "    # print('X_train_cv shape:', X_train_cv.shape)\n",
        "    # print('y_train_cv shape:', y_train_cv.shape)\n",
        "    # print('X_valid_cv shape:', X_valid_cv.shape)\n",
        "    # print('y_valid_cv shape:', y_valid_cv.shape)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n#Check results of k-fold split\\nfor j, (train_idx, val_idx) in enumerate(folds):\\n    print('\\nFold ',j)\\n    print('train_idx = ',train_idx)\\n    print('val_idx = ',val_idx)\\n    X_train_cv = final_seq[train_idx]\\n    y_train_cv = target[train_idx]\\n    X_valid_cv = final_seq[val_idx]\\n    y_valid_cv = target[val_idx]\\n\\n    # print('X_train_cv shape:', X_train_cv.shape)\\n    # print('y_train_cv shape:', y_train_cv.shape)\\n    # print('X_valid_cv shape:', X_valid_cv.shape)\\n    # print('y_valid_cv shape:', y_valid_cv.shape)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKykHIx3_I7h",
        "colab_type": "text"
      },
      "source": [
        "# Checkpoint: Environment Initialised"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SU-Md-2y_NYM",
        "colab_type": "code",
        "outputId": "a41f7348-b2f5-4bec-dda8-4672c9382d38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Environment Initialised Sucessfully\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Environment Initialised Sucessfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCSJzhPA7RaO",
        "colab_type": "text"
      },
      "source": [
        "# Define model and train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM13WDe2Kzd_",
        "colab_type": "text"
      },
      "source": [
        "## 1_Best Model\n",
        "(added maxpooling/averagepooling layer, regularization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Uh53I7NUx1G",
        "colab_type": "code",
        "outputId": "28391523-20d3-4ed8-8337-0adbdabef46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "model_id = 1\n",
        "\n",
        "backend.clear_session()\n",
        "\n",
        "#input layer\n",
        "visible = Input(shape=(1000,74))\n",
        "\n",
        "#2 separate convolution1 layers\n",
        "left_conv1 = Conv1D(256, kernel_size=4, strides=2, padding='same')(visible)\n",
        "right_conv1 = Conv1D(256, kernel_size=4, strides=2, padding='same', activation='sigmoid')(visible)\n",
        "merge = concatenate([left_conv1, right_conv1])\n",
        "gru = GRU(128, dropout=0.3, recurrent_dropout=0.3, \n",
        "          activation=\"sigmoid\",recurrent_activation=\"sigmoid\")(merge)\n",
        "          # activity_regularizer=regularizers.l1(0.01))(merge)\n",
        "output = Dense(1, activation='sigmoid')(gru)\n",
        "model = Model(inputs=[visible], outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1000, 74)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 256)     76032       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 256)     76032       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 500, 512)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 128)          246144      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            129         gru_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 398,337\n",
            "Trainable params: 398,337\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVZ1ppoU2mr",
        "colab_type": "text"
      },
      "source": [
        "## 2_ Add additional GRU, reduce feature maps sizes\n",
        "Kaggle score only 0.982. Bad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GocTOr0S9lKa",
        "colab_type": "code",
        "outputId": "87d10942-88b6-46b4-c5a1-db590a6afda4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "model_id = 2\n",
        "\n",
        "backend.clear_session()\n",
        "\n",
        "#input layer\n",
        "visible = Input(shape=(1000,74))\n",
        "\n",
        "#2 separate convolution1 layers\n",
        "left_conv1 = Conv1D(128, kernel_size=4, strides=2, padding='same')(visible)\n",
        "right_conv1 = Conv1D(128, kernel_size=4, strides=2, padding='same', activation='sigmoid')(visible)\n",
        "merge = concatenate([left_conv1, right_conv1])\n",
        "gru = GRU(64, dropout=0.3, recurrent_dropout=0.3, activation=\"sigmoid\",recurrent_activation=\"sigmoid\", return_sequences=True)(merge)\n",
        "gru2 = GRU(64, dropout=0.3, recurrent_dropout=0.3, activation=\"sigmoid\",recurrent_activation=\"sigmoid\")(gru)\n",
        "output = Dense(1, activation='sigmoid')(gru2)\n",
        "model = Model(inputs=[visible], outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1000, 74)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 128)     38016       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 128)     38016       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 500, 256)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "gru_1 (GRU)                     (None, 500, 64)      61632       concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     (None, 64)           24768       gru_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            65          gru_2[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 162,497\n",
            "Trainable params: 162,497\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo-8S82o2lwm",
        "colab_type": "text"
      },
      "source": [
        "## 3_Bidirectional GRU 2nd one (score 0.98, model=3)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQZ8N4GS2qFy",
        "colab_type": "code",
        "outputId": "641d4779-7049-43c5-a416-ecc73e119006",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "model_id = 3\n",
        "\n",
        "backend.clear_session()\n",
        "\n",
        "#input layer\n",
        "visible = Input(shape=(1000,74))\n",
        "\n",
        "#2 separate convolution1 layers\n",
        "left_conv1 = Conv1D(256, kernel_size=4, strides=2, padding='same')(visible)\n",
        "right_conv1 = Conv1D(256, kernel_size=4, strides=2, padding='same', activation='sigmoid')(visible)\n",
        "merge = concatenate([left_conv1, right_conv1])\n",
        "gru = Bidirectional(GRU(128, dropout=0.3, recurrent_dropout=0.3, \n",
        "          activation=\"sigmoid\",recurrent_activation=\"sigmoid\"))(merge)\n",
        "          # activity_regularizer=regularizers.l1(0.01)))(merge)\n",
        "output = Dense(1, activation='sigmoid')(gru)\n",
        "model = Model(inputs=[visible], outputs=output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:107: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:111: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 1000, 74)     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, 500, 256)     76032       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_2 (Conv1D)               (None, 500, 256)     76032       input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 500, 512)     0           conv1d_1[0][0]                   \n",
            "                                                                 conv1d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, 256)          492288      concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1)            257         bidirectional_1[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 644,609\n",
            "Trainable params: 644,609\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUZ4irXC_b1_",
        "colab_type": "text"
      },
      "source": [
        "## from andy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxz8yFyR_ECs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv1D(128, input_shape=(1000, 74), activation='relu', kernel_size=4, strides=2, padding='same')) \n",
        "model.add(GRU(128, dropout=0.2, activation='hard_sigmoid', recurrent_activation='hard_sigmoid', input_shape=(1000, 74))) \n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wzxk6zsF9iCd",
        "colab_type": "text"
      },
      "source": [
        "### Model 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiXV6ql1K2Ha",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv1D(512, input_shape=(None, 102), kernel_size=2, strides=2, padding='same', use_bias=True)) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(GRU(512, dropout=0.3, return_sequences=True, use_bias=True))\n",
        "model.add(GRU(256, dropout=0.2,return_sequences=True, use_bias=True))\n",
        "model.add(Conv1D(256, kernel_size=2, strides=2, padding='same', use_bias=True)) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(GRU(256, dropout=0.3, return_sequences=True, use_bias=True)) \n",
        "model.add(GRU(128, dropout=0.2, use_bias=True)) \n",
        "# model.add(GlobalAvgPooling)      try later\n",
        "# model.add(Dropout(0.3))\n",
        "# model.add(Dense(64, activation=\"sigmoid\", use_bias=True))\n",
        "model.add(Dense(1, activation=\"sigmoid\", use_bias=True))\n",
        "# model.add(Softmax layer)   try later. from lecnotes\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BA5sRvquCIo",
        "colab_type": "text"
      },
      "source": [
        "## Other tries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwGWYMmVkeE3",
        "colab_type": "text"
      },
      "source": [
        "### With embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FaoVGWakdLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Embedding(10000, 64, input_length=102))\n",
        "model.add(Conv1D(256, input_shape=(None, 74), kernel_size=2, strides=2, padding='same')) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(Bidirectional(GRU(256, dropout=0.3, return_sequences=True, input_shape=(1000, 74))))\n",
        "model.add(Bidirectional(GRU(128,dropout=0.2,return_sequences=True)))\n",
        "model.add(Conv1D(256, input_shape=(1000, 74), kernel_size=2, strides=2, padding='same')) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(GRU(256, dropout=0.2, return_sequences=True, input_shape=(1000, 74))) \n",
        "model.add(GRU(128)) \n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYFzQVcpgvGQ",
        "colab_type": "text"
      },
      "source": [
        "### Sequential Model 1\n",
        "Conv1D -> MaxPool -> GRUx2 -> Densex2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hP0VGn6VJy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Build time series classification model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(256, input_shape=(1000, 102), kernel_size=2, strides=2, padding='same'))\n",
        "# model.add(Conv1D(128, kernel_size=2, strides=1, padding='valid'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "model.add(GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
        "model.add(GRU(128, dropout=0.3, recurrent_dropout=0.3))\n",
        "model.add(Dense(32, activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiRlSB8hbjQV",
        "colab_type": "text"
      },
      "source": [
        "### Sequential Model 2\n",
        "Conv1D -> Conv1D -> MaxPool -> Bidirection(GRU)x2 -> Densex2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHQMk6HabgEM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Build time series classification model\n",
        "model = Sequential()\n",
        "model.add(Conv1D(256, input_shape=(1000, 102), kernel_size=2, strides=2, padding='same'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides=2))\n",
        "model.add(Conv1D(128, kernel_size=2, strides=1, padding='valid'))\n",
        "model.add(MaxPooling1D(pool_size=2, strides=1))\n",
        "model.add(Bidirectional(GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)))\n",
        "# model.add(Dropout(0.5))\n",
        "model.add(Bidirectional(GRU(128, dropout=0.3, recurrent_dropout=0.3, )))\n",
        "model.add(Dense(32, activation=\"sigmoid\"))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex9ZQ7aH-cMt",
        "colab_type": "text"
      },
      "source": [
        "### Sequential Model 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-H4STZk-hRX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv1D(256, input_shape=(1000, 102), kernel_size=2, strides=2, padding='same')) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(GRU(256, dropout=0.3, return_sequences=True, input_shape=(1000, 74))) \n",
        "model.add(GRU(128,dropout=0.2,return_sequences=True)) \n",
        "model.add(Conv1D(256, input_shape=(1000, 74), kernel_size=2, strides=2, padding='same')) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(GRU(256, dropout=0.2, return_sequences=True, input_shape=(1000, 74))) \n",
        "model.add(GRU(128)) \n",
        "model.add(Dense(1, activation=\"sigmoid\")) \n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZhkxEHoLnf4",
        "colab_type": "text"
      },
      "source": [
        "### Sequential 4 (02Nov2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M00092nsLrlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = Sequential()\n",
        "# Prevent overfitting using dropout method of regularization\n",
        "model.add(Dropout(0.5))\n",
        "model.add(LSTM(64, input_shape=(1000, 102), recurrent_dropout=0.5))\n",
        "model.add(Dropout(0.5))\n",
        "# Condense to single binary output value\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SK0wzAiCP6DJ",
        "colab_type": "text"
      },
      "source": [
        "### Sequential 5 (02Nov 2019)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeqhtiVTP8zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "backend.clear_session()\n",
        "\n",
        "model = Sequential() \n",
        "model.add(Conv1D(256, input_shape=(None, 74), kernel_size=2, strides=2, padding='same')) \n",
        "model.add(MaxPooling1D(pool_size=2, strides=2)) \n",
        "model.add(LSTM(256, dropout=0.3, return_sequences=True, use_bias=True, input_shape=(1000, 74)))\n",
        "model.add(LSTM(128, dropout=0.2))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0VKUPDOVHPf",
        "colab_type": "text"
      },
      "source": [
        "### Functional Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPC4RyVYIY35",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input, Flatten, ReLU\n",
        "from keras.layers.merge import concatenate\n",
        "from keras.models import Model\n",
        "\n",
        "backend.clear_session()\n",
        "\n",
        "#input layer\n",
        "visible = Input(shape=(1000,102))\n",
        "conv1 = Conv1D(256, input_shape=(1000, 102), kernel_size=2, strides=2, padding='same')(visible)\n",
        "pool1 = MaxPooling1D(pool_size=2, strides=2)(conv1)\n",
        "gru1 = GRU(256, dropout=0.3, recurrent_dropout=0.3, return_sequences=True)(pool1)\n",
        "gru2 = GRU(128,dropout=0.2, recurrent_dropout=0.3)(gru1)\n",
        "dense1 = Dense(64, activation='sigmoid')(gru2)\n",
        "\n",
        "dense2 = Dense(32, activation='sigmoid')(dense1)\n",
        "dense3 = Dense(32, activation='sigmoid')(dense1)\n",
        "dense4 = Dense(10, activation='sigmoid')(dense3)\n",
        "\n",
        "merge = concatenate([dense2, dense4])\n",
        "output = Dense(1, activation='sigmoid')(merge)\n",
        "\n",
        "'''\n",
        "#2 separate convolution layers\n",
        "left_conv = Conv1D(128, kernel_size=2, strides=2, padding='same')(visible)\n",
        "right_conv = Conv1D(128, kernel_size=2, strides=2, padding='same', activation='sigmoid')(visible)\n",
        "\n",
        "left_pool = MaxPooling1D(pool_size=2, strides=2)(left_conv)\n",
        "right_pool = MaxPooling1D(pool_size=2, strides=2)(right_conv)\n",
        "\n",
        "left_gru = GRU(256, dropout=0.3, recurrent_dropout=0.3)(left_pool) \n",
        "right_gru = GRU(256, dropout=0.3, recurrent_dropout=0.3)(left_pool) \n",
        "\n",
        "#left_flat = Flatten()(left_pool)\n",
        "#right_flat = Flatten()(right_pool)\n",
        "\n",
        "#merge convolution layers\n",
        "merge = concatenate([left_gru, right_gru])\n",
        "relu = ReLU()(merge)\n",
        "# pool = MaxPooling1D(pool_size=2, strides=2)(relu)\n",
        "\n",
        "# gru1 = GRU(256, dropout=0.2, return_sequences=True)(pool) \n",
        "# gru2 = GRU(128, dropout=0.2)(gru1)\n",
        "\n",
        "# flat = Flatten()(pool)\n",
        "dense1 = Dense(128, activation='relu')(relu)\n",
        "output = Dense(1, activation='softmax')(dense1)\n",
        "\n",
        "'''\n",
        "\n",
        "model = Model(inputs=[visible], outputs=output)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY2XIIv_vvyU",
        "colab_type": "text"
      },
      "source": [
        "## Define callbacks and custom functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q_s53p1U3dj",
        "colab_type": "text"
      },
      "source": [
        "### Custom function - Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRglDXpWKob4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epoch = 10\n",
        "\n",
        "class XTensorBoard(TensorBoard):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      print('Learning rate = ', backend.eval(self.model.optimizer.lr))\n",
        "\n",
        "class OptimizerChanger(EarlyStopping):\n",
        "    def __init__(self, on_train_end, **kwargs):\n",
        "        self.do_on_train_end = on_train_end\n",
        "        super(OptimizerChanger,self).__init__(**kwargs)\n",
        "\n",
        "    def on_train_end(self, logs=None):\n",
        "        # super(OptimizerChanger,self).on_train_end(self,logs)\n",
        "        print('Switching to SGD Optimizer')\n",
        "        ## TODO ##\n",
        "        # reset the learning rate for SGD to 0.0005\n",
        "        super(OptimizerChanger,self).on_train_end(logs)\n",
        "        self.do_on_train_end()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LP89z9qU8dt",
        "colab_type": "text"
      },
      "source": [
        "### Create Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mN2BacjZvvHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sgd = SGD(lr=0.0005, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "\n",
        "# 1. print learning rate\n",
        "cb_print_lr = XTensorBoard()\n",
        "\n",
        "# 2. define callback - reduce learning rate on plateau\n",
        "cb_reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, min_lr=0.000001)\n",
        "\n",
        "# 3. define callback - early stopping\n",
        "cb_early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=4, restore_best_weights=True)\n",
        "\n",
        "# 4. define callback - model checkpoint\n",
        "filepath = \"/content/drive/My Drive/NUS_2019/cs5242_kaggle/models/\"+str(model_id)+\"_model-{epoch:02d}-{val_acc:.4f}.hdf5\"\n",
        "cb_checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
        "\n",
        "callback_op_change = [cb_print_lr, cb_reduce_lr, cb_early_stopping, cb_checkpoint]\n",
        "\n",
        "# 5. define callback - change to SGD if acc does not improve\n",
        "def do_after_training():\n",
        "    #warining, this creates a new optimizer and,\n",
        "    #at the beginning, it might give you a worse training performance than before\n",
        "    adam = Adam(lr=0.0005)\n",
        "    model.compile(optimizer = adam, loss='binary_crossentropy', metrics=['acc'])\n",
        "    model.fit(X_train_cv, y_train_cv, \n",
        "                        epochs=1,\n",
        "                        batch_size=64,\n",
        "                        validation_data=(X_valid_cv, y_valid_cv), \n",
        "                        callbacks=callback_op_change)\n",
        "    \n",
        "changer = OptimizerChanger(on_train_end=do_after_training, \n",
        "                           monitor='val_acc',\n",
        "                           min_delta=0.005,\n",
        "                           patience=1)\n",
        "\n",
        "# define callback - history\n",
        "history = History()\n",
        "\n",
        "# cbks = [history, cb_print_lr, cb_reduce_lr, cb_checkpoint, changer]\n",
        "cbks = [history, cb_print_lr, cb_reduce_lr, cb_checkpoint, cb_early_stopping]\n",
        "\n",
        "# define callback - learning rate scheduler (replaced with reduceLRonPlateau)\n",
        "# reducing the learning rate by half every 2 epochs\n",
        "# cbks = [LearningRateScheduler(lambda epoch: 0.001 * 0.5 ** (epoch // 2)),\n",
        "#         TensorBoard(write_graph=False)]\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtkSeHZzb6Up",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Compile and Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWTFKFkHs--d",
        "colab_type": "text"
      },
      "source": [
        "## Train with k fold CV (batchsize = 128)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxvdQIaZtBAi",
        "colab_type": "code",
        "outputId": "cd54e784-0ef8-4fd8-b924-24340d1be8d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "    print('\\nFold ',j)\n",
        "    \n",
        "    X_train_cv = final_seq[train_idx]\n",
        "    y_train_cv = target[train_idx]\n",
        "    X_valid_cv = final_seq[val_idx]\n",
        "    y_valid_cv = target[val_idx]\n",
        "\n",
        "    # print_lr = MyCallBack()\n",
        "    \n",
        "    cbks = [history, cb_print_lr, cb_reduce_lr, cb_checkpoint, cb_early_stopping]\n",
        "    # train the model\n",
        "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    #define early stopping to stop when validation is not decreasing anymore\n",
        "    # early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, restore_best_weights=True)\n",
        "\n",
        "    ## TODO ##\n",
        "    # 1. Decrease batch size for Adam every n epochs\n",
        "    # 2. Decrease learning rate for Adam every n epochs\n",
        "    # 3. Reset learning rate for SGD every n epochs / or just change to Adam\n",
        "\n",
        "    history = model.fit(X_train_cv, y_train_cv, \n",
        "                        epochs=2,\n",
        "                        batch_size=128,\n",
        "                        validation_data=(X_valid_cv, y_valid_cv), \n",
        "                        callbacks=cbks)\n",
        "\n",
        "    # print(model.evaluate(X_valid_cv, y_valid_cv))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold  0\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 16795 samples, validate on 1867 samples\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/2\n",
            "16795/16795 [==============================] - 288s 17ms/step - loss: 0.3640 - acc: 0.8326 - val_loss: 0.2749 - val_acc: 0.8784\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.87841, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.8784.hdf5\n",
            "Epoch 2/2\n",
            "16795/16795 [==============================] - 288s 17ms/step - loss: 0.2543 - acc: 0.8950 - val_loss: 0.2207 - val_acc: 0.9138\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.87841 to 0.91377, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-02-0.9138.hdf5\n",
            "\n",
            "Fold  1\n",
            "Train on 16795 samples, validate on 1867 samples\n",
            "Epoch 1/2\n",
            "16795/16795 [==============================] - 291s 17ms/step - loss: 0.2101 - acc: 0.9166 - val_loss: 0.1885 - val_acc: 0.9250\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.91377 to 0.92501, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9250.hdf5\n",
            "Epoch 2/2\n",
            "16795/16795 [==============================] - 289s 17ms/step - loss: 0.1820 - acc: 0.9266 - val_loss: 0.1729 - val_acc: 0.9325\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.92501 to 0.93251, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-02-0.9325.hdf5\n",
            "\n",
            "Fold  2\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/2\n",
            "16796/16796 [==============================] - 292s 17ms/step - loss: 0.1680 - acc: 0.9322 - val_loss: 0.1602 - val_acc: 0.9330\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.93251 to 0.93301, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9330.hdf5\n",
            "Epoch 2/2\n",
            "16796/16796 [==============================] - 292s 17ms/step - loss: 0.1525 - acc: 0.9415 - val_loss: 0.1665 - val_acc: 0.9335\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.93301 to 0.93355, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-02-0.9335.hdf5\n",
            "\n",
            "Fold  3\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/2\n",
            "16796/16796 [==============================] - 289s 17ms/step - loss: 0.1485 - acc: 0.9408 - val_loss: 0.1233 - val_acc: 0.9528\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.93355 to 0.95284, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9528.hdf5\n",
            "Epoch 2/2\n",
            "16796/16796 [==============================] - 281s 17ms/step - loss: 0.1315 - acc: 0.9497 - val_loss: 0.1230 - val_acc: 0.9582\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.95284 to 0.95820, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-02-0.9582.hdf5\n",
            "\n",
            "Fold  4\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/2\n",
            "16796/16796 [==============================] - 282s 17ms/step - loss: 0.1308 - acc: 0.9496 - val_loss: 0.0972 - val_acc: 0.9641\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.95820 to 0.96409, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9641.hdf5\n",
            "Epoch 2/2\n",
            "12416/16796 [=====================>........] - ETA: 1:09 - loss: 0.1188 - acc: 0.9560"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42tE5PlsDU3B",
        "colab_type": "code",
        "outputId": "fcde0d77-391c-4d17-a8fb-e48b3cf2d466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "    print('\\nFold ',j)\n",
        "    \n",
        "    X_train_cv = final_seq[train_idx]\n",
        "    y_train_cv = target[train_idx]\n",
        "    X_valid_cv = final_seq[val_idx]\n",
        "    y_valid_cv = target[val_idx]\n",
        "    \n",
        "    predict_data = model.predict(X_valid_cv, batch_size=128)\n",
        "\n",
        "    print('Accuracy Score : ' + str(accuracy_score(y_valid_cv,predict_data.round())))\n",
        "    print('Precision Score : ' + str(precision_score(y_valid_cv,predict_data.round())))\n",
        "    print('Recall Score : ' + str(recall_score(y_valid_cv,predict_data.round())))\n",
        "    print('F1 Score : ' + str(f1_score(y_valid_cv,predict_data.round())))\n",
        "\n",
        "    #Dummy Classifier Confusion matrix\n",
        "    print('Confusion Matrix : \\n' + str(confusion_matrix(y_valid_cv,predict_data.round())))\n",
        "\n",
        "    break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold  0\n",
            "Accuracy Score : 0.5634708087841457\n",
            "Precision Score : 0.5634708087841457\n",
            "Recall Score : 1.0\n",
            "F1 Score : 0.7207947927372388\n",
            "Confusion Matrix : \n",
            "[[   0  815]\n",
            " [   0 1052]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6d0Axo3W4WS",
        "colab_type": "text"
      },
      "source": [
        "## Reduce batch size and train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkSXrJxxW33N",
        "colab_type": "code",
        "outputId": "92fc969c-8dc1-4c37-c0df-5df2a34ab5a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for j, (train_idx, val_idx) in enumerate(folds):\n",
        "    print('\\nFold ',j)\n",
        "    \n",
        "    #reset optmizers with each k\n",
        "    # adam = Adam(lr=0.0005)\n",
        "    # sgd = SGD(lr=0.0005, momentum=0.8)\n",
        "\n",
        "    # callbacks_1 = [history, cb_print_lr, cb_reduce_lr, cb_checkpoint, changer]\n",
        "\n",
        "    X_train_cv = final_seq[train_idx]\n",
        "    y_train_cv = target[train_idx]\n",
        "    X_valid_cv = final_seq[val_idx]\n",
        "    y_valid_cv = target[val_idx]\n",
        "\n",
        "    # print_lr = MyCallBack()\n",
        "    cbks = [history, cb_print_lr, cb_reduce_lr, cb_checkpoint, changer]\n",
        "\n",
        "    # train the model\n",
        "    model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['acc'])\n",
        "\n",
        "    #define early stopping to stop when validation is not decreasing anymore\n",
        "    # early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=5, restore_best_weights=True)\n",
        "\n",
        "    ## TODO ##\n",
        "    # 1. Decrease batch size for Adam every n epochs\n",
        "    # 2. Decrease learning rate for Adam every n epochs\n",
        "    # 3. Reset learning rate for SGD every n epochs / or just change to Adam\n",
        "\n",
        "    history = model.fit(X_train_cv, y_train_cv, \n",
        "                        epochs=1,\n",
        "                        batch_size=128,\n",
        "                        validation_data=(X_valid_cv, y_valid_cv), \n",
        "                        callbacks=cbks)\n",
        "\n",
        "    # print(model.evaluate(X_valid_cv, y_valid_cv))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Fold  0\n",
            "Train on 16795 samples, validate on 1867 samples\n",
            "Epoch 1/1\n",
            "16795/16795 [==============================] - 291s 17ms/step - loss: 0.0898 - acc: 0.9684 - val_loss: 0.0905 - val_acc: 0.9630\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.97374\n",
            "Switching to SGD Optimizer\n",
            "Train on 16795 samples, validate on 1867 samples\n",
            "Epoch 1/1\n",
            "16795/16795 [==============================] - 425s 25ms/step - loss: 0.0821 - acc: 0.9713 - val_loss: 0.0753 - val_acc: 0.9721\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.97374\n",
            "\n",
            "Fold  1\n",
            "Train on 16795 samples, validate on 1867 samples\n",
            "Epoch 1/1\n",
            "16795/16795 [==============================] - 288s 17ms/step - loss: 0.0857 - acc: 0.9686 - val_loss: 0.0728 - val_acc: 0.9770\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.97374 to 0.97697, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9770.hdf5\n",
            "Switching to SGD Optimizer\n",
            "Train on 16795 samples, validate on 1867 samples\n",
            "Epoch 1/1\n",
            "16795/16795 [==============================] - 431s 26ms/step - loss: 0.0774 - acc: 0.9720 - val_loss: 0.0764 - val_acc: 0.9770\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.97697\n",
            "\n",
            "Fold  2\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 287s 17ms/step - loss: 0.0806 - acc: 0.9705 - val_loss: 0.0730 - val_acc: 0.9700\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.97697\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 429s 26ms/step - loss: 0.0743 - acc: 0.9736 - val_loss: 0.0758 - val_acc: 0.9689\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.97697\n",
            "\n",
            "Fold  3\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 288s 17ms/step - loss: 0.0770 - acc: 0.9724 - val_loss: 0.0791 - val_acc: 0.9748\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.97697\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 428s 26ms/step - loss: 0.0713 - acc: 0.9756 - val_loss: 0.0640 - val_acc: 0.9796\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.97697 to 0.97964, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9796.hdf5\n",
            "\n",
            "Fold  4\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 292s 17ms/step - loss: 0.0772 - acc: 0.9720 - val_loss: 0.0499 - val_acc: 0.9861\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc improved from 0.97964 to 0.98607, saving model to /content/drive/My Drive/NUS_2019/cs5242_kaggle/models/3_model-01-0.9861.hdf5\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 434s 26ms/step - loss: 0.0711 - acc: 0.9755 - val_loss: 0.0505 - val_acc: 0.9845\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "\n",
            "Fold  5\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 292s 17ms/step - loss: 0.0721 - acc: 0.9740 - val_loss: 0.0657 - val_acc: 0.9786\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 430s 26ms/step - loss: 0.0653 - acc: 0.9771 - val_loss: 0.0682 - val_acc: 0.9764\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "\n",
            "Fold  6\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 295s 18ms/step - loss: 0.0702 - acc: 0.9755 - val_loss: 0.0728 - val_acc: 0.9791\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 431s 26ms/step - loss: 0.0640 - acc: 0.9779 - val_loss: 0.0711 - val_acc: 0.9764\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "\n",
            "Fold  7\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 292s 17ms/step - loss: 0.0693 - acc: 0.9763 - val_loss: 0.0533 - val_acc: 0.9802\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 430s 26ms/step - loss: 0.0624 - acc: 0.9790 - val_loss: 0.0550 - val_acc: 0.9807\n",
            "Learning rate =  0.0005\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "\n",
            "Fold  8\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "16796/16796 [==============================] - 294s 17ms/step - loss: 0.0663 - acc: 0.9772 - val_loss: 0.0557 - val_acc: 0.9802\n",
            "Learning rate =  0.001\n",
            "\n",
            "Epoch 00001: val_acc did not improve from 0.98607\n",
            "Switching to SGD Optimizer\n",
            "Train on 16796 samples, validate on 1866 samples\n",
            "Epoch 1/1\n",
            "14720/16796 [=========================>....] - ETA: 52s - loss: 0.0599 - acc: 0.9793"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-41610d9a3d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                         callbacks=cbks)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# print(model.evaluate(X_valid_cv, y_valid_cv))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_end_hook\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Helper function for on_{train|test|predict}_end methods.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_TEST\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \"\"\"\n\u001b[1;32m    228\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-222a1ca3381a>\u001b[0m in \u001b[0;36mon_train_end\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# reset the learning rate for SGD to 0.0005\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOptimizerChanger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_on_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-91de7dc02400>\u001b[0m in \u001b[0;36mdo_after_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                         callbacks=callback_op_change)\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m changer = OptimizerChanger(on_train_end=do_after_training, \n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    202\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lnhgHycSZMc",
        "colab_type": "code",
        "outputId": "3fa21bc0-d50c-45e3-e18c-098e81af1052",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(model.metrics_names)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['loss', 'acc']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsSjstZKuT0I",
        "colab_type": "text"
      },
      "source": [
        "## Train without k-fold CV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0fzVifLVLjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "# train the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#define early stopping to stop when validation is not decreasing anymore\n",
        "# early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.005, patience=20, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=100,\n",
        "                    batch_size=128,\n",
        "                    validation_data=(X_valid, y_valid))\n",
        "                    # , \n",
        "                    # callbacks=[early_stopping])\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGCNUzGMudL1",
        "colab_type": "text"
      },
      "source": [
        "# Results and Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_eGihaWNFAk",
        "colab_type": "text"
      },
      "source": [
        "## Load best model from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfgSovUBNKjO",
        "colab_type": "code",
        "outputId": "28427378-74d0-49e2-8104-07bc4935cda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model_path = '/content/drive/My Drive/NUS_2019/cs5242_kaggle/models/'\n",
        "model_fn = 'run_20191107/2_model-02-0.9866.hdf5'\n",
        "\n",
        "model.compile(optimizer='Adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                   metrics=['acc'])\n",
        "\n",
        "model.load_weights(model_path+model_fn)\n",
        "print(\"Model Loaded\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model Loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrMPSISdEGGz",
        "colab_type": "text"
      },
      "source": [
        "## Get predictions for Test data (count=6501)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAfnGIZbD3XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read all training sequence for all instances\n",
        "path = '/content/kaggle/test/'\n",
        "sequences = list()\n",
        "\n",
        "for i in range(0, len(listdir(path))):\n",
        "    file_name = path + str(i) + '.npy'\n",
        "    data = np.load(file_name)\n",
        "    df = pd.DataFrame(data=data[:,:], index=None, columns=None)\n",
        "    df = df.drop(df.iloc[:,64:92].head(0).columns, axis=1)\n",
        "    values = df.values\n",
        "    sequences.append(values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBJwnYenEDM1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# padding the sequence with values from last row for each sequence\n",
        "# since most of the sequence are 1000 length, no need to truncate further after padding\n",
        "\n",
        "to_pad = 1000\n",
        "new_sequences = []\n",
        "for seq in sequences:\n",
        "    len_seq = len(seq)\n",
        "    # last_val = seq[-1]\n",
        "    last_val = [0]*74\n",
        "    n = to_pad - len_seq\n",
        "  \n",
        "    '''\n",
        "    to_concat = np.repeat(seq[-1], n).reshape(102, n).transpose()\n",
        "    new_seq = np.concatenate([seq, to_concat])\n",
        "    new_sequences.append(new_seq)\n",
        "    '''\n",
        "    to_concat = np.repeat(last_val, n).reshape(74, n).transpose()\n",
        "    new_seq = np.concatenate([seq, to_concat])\n",
        "    new_sequences.append(new_seq)\n",
        "    \n",
        "final_seq_test = np.stack(new_sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHxV0rDnEPps",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save the model, with the validation score and timestamp\n",
        "now = datetime.now(timezone('Asia/Singapore'))\n",
        "curr_timestamp = now.strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "test_data = np.array(final_seq_test)\n",
        "predict_data = model.predict(test_data, batch_size=128)\n",
        "\n",
        "#save solutions file to Google Drive\n",
        "sol_path = '/content/drive/My Drive/NUS_2019/cs5242_kaggle/models/solution_' + curr_timestamp + '.csv'\n",
        "predict_df = pd.DataFrame(data=np.array(predict_data), columns=['Predicted']).to_csv(sol_path, index_label='Id')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VgvJSdPEpqj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_df_1 = pd.DataFrame(data=np.array(predict_data), columns=['Predicted'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAk_yVxhQiSr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_df_1['Predicted_Round'] = predict_df_1['Predicted'].round(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTEJ7PkHQoLZ",
        "colab_type": "code",
        "outputId": "b5348d71-6bfd-4d51-b7fd-c91a7e37dd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "predict_df_1.sample(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Predicted_Round</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3384</th>\n",
              "      <td>0.965190</td>\n",
              "      <td>0.965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>0.994970</td>\n",
              "      <td>0.995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4815</th>\n",
              "      <td>0.999993</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2361</th>\n",
              "      <td>0.999993</td>\n",
              "      <td>1.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4440</th>\n",
              "      <td>0.321670</td>\n",
              "      <td>0.322</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Predicted  Predicted_Round\n",
              "3384   0.965190            0.965\n",
              "433    0.994970            0.995\n",
              "4815   0.999993            1.000\n",
              "2361   0.999993            1.000\n",
              "4440   0.321670            0.322"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWFtqp9tQ_vP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}